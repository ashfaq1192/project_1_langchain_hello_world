{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJmHAPsPqjObeUA95Tdmzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfaq1192/project_1_langchain_hello_world/blob/main/langchain_hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project No.1: Langchain Hello World"
      ],
      "metadata": {
        "id": "vtoLvHlRnoIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submitted By : Muhammad Ashfaq (ID: PIAIC209123)"
      ],
      "metadata": {
        "id": "D75iCCYtoB8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What is Langchain*"
      ],
      "metadata": {
        "id": "1NCl949vaTJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " That is middle layer framework that connects any LLM from backend.It is a framework used to build LLM based applications with all components like database, tool calling, connectivity, memory, rag and many other you can do. Whenever you need to change any component in AI Comound system, you just change only that specific component."
      ],
      "metadata": {
        "id": "1oMuRXKwbUfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing packages:  Langchain and Google Generative AI"
      ],
      "metadata": {
        "id": "PuTjLXCjfcsg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68Hq3BxZ33s"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will Import the SDK"
      ],
      "metadata": {
        "id": "o5o3PLc3flfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_google_genai as genai"
      ],
      "metadata": {
        "id": "yFmBW-web2TE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Google Gemini LLM Class"
      ],
      "metadata": {
        "id": "_0Up1-lvcbPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "SUX_POyccMC2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Connected with AI Studio with our API"
      ],
      "metadata": {
        "id": "GBRbT8Zzf16I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "diVR6fz1cvif"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already imported the class \"GoogleGenerativeAI\", in this class we will pass two things. Model Name and API Key. By passing these two things, now we have object of our Langchain model"
      ],
      "metadata": {
        "id": "49AXb9UVhGkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm : ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model = 'gemini-2.0-flash-exp',\n",
        "    api_key = GOOGLE_API_KEY,\n",
        "    temperature = 0.5,\n",
        "    max_tokens= 200\n",
        ")"
      ],
      "metadata": {
        "id": "hVxePdmeclC1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = llm.invoke(\"Wht is the capital of Nepal?\")"
      ],
      "metadata": {
        "id": "3kEW8NRle6fE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6BB4sNofJxL",
        "outputId": "6b2410e9-0b24-47f7-98fb-f701c4450c9b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Nepal is **Kathmandu**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making and Chain and Prompt Template"
      ],
      "metadata": {
        "id": "V1ikcTCiKzFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
        "#RunnableSequence: is used to create a pipeline of operations (a chain) that can be executed.\n",
        "#RunnablePassthrough: is used to pass input data through a step in the pipeline without modification.\n",
        "import os\n",
        "# Define the prompt template\n",
        "#PromptTemplate is used to create a reusable template for generating prompts for language models.\n",
        "#It allows you to define placeholders for variables that will be filled in later.\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the chain using RunnableSequence (the new approach)\n",
        "chain =  (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")\n",
        "# Run the chain with a sample question (using invoke)\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.invoke({\"question\": question})\n",
        "print(\"Answer:\", response.content)"
      ],
      "metadata": {
        "id": "AdemObxN7g3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding More Tempaltes"
      ],
      "metadata": {
        "id": "LPQHsu-bM3lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
        "import os\n",
        "# Define multiple prompt templates\n",
        "prompt_templates = [\n",
        "    PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        "    ),\n",
        "    PromptTemplate(\n",
        "        input_variables=[\"question\"],\n",
        "        template=\"Respond to the following question in a concise manner:\\n\\n{question}\"\n",
        "    )\n",
        "]\n",
        "#Question\n",
        "question = \"What is the purpose of Liver in human body?\"\n",
        "\n",
        "# Loop through each prompt template and get a response\n",
        "for i, prompt_template in enumerate(prompt_templates):\n",
        "    # Create the chain using RunnableSequence\n",
        "    chain = (\n",
        "        {\"question\": RunnablePassthrough()}\n",
        "        | prompt_template\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    # Run the chain with the question\n",
        "    response = chain.invoke({\"question\": question})\n",
        "\n",
        "    # Print the response\n",
        "    print(f\"--- Response with Prompt Template {i+1} ---\")\n",
        "    print(\"Answer:\", response.content)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJpIYWQqM2jA",
        "outputId": "ea90baf2-ba3f-42d9-f424-227d0c203668"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Response with Prompt Template 1 ---\n",
            "Answer: The liver is a vital organ with a wide range of functions, making it essential for human survival. Here are some of its key purposes:\n",
            "\n",
            "**1. Filtration and Detoxification:**\n",
            "\n",
            "*   **Filtering Blood:** The liver filters blood coming from the digestive tract, removing toxins, bacteria, and other harmful substances.\n",
            "*   **Detoxifying Chemicals:** It breaks down and neutralizes drugs, alcohol, and other harmful chemicals, converting them into less toxic forms that can be excreted by the body.\n",
            "\n",
            "**2. Metabolism:**\n",
            "\n",
            "*   **Carbohydrate Metabolism:** The liver plays a crucial role in regulating blood sugar levels. It stores glucose as glycogen and releases it when needed. It also converts other sugars into glucose.\n",
            "*   **Protein Metabolism:** The liver synthesizes many important proteins, including albumin (for maintaining fluid balance) and clotting factors (for blood coagulation). It also breaks down proteins and produces urea, a waste product.\n",
            "*   **Fat Metabolism:** The\n",
            "\n",
            "--- Response with Prompt Template 2 ---\n",
            "Answer: The liver filters blood, produces bile, and metabolizes substances.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}